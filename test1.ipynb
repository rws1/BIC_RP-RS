{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup imports \n",
    "# created with guidance from https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "def file_read(file_name):\n",
    "    with open(file_name, 'r') as data:\n",
    "        for line in data:\n",
    "            p = line.split()\n",
    "            x.append(float(p[0]))\n",
    "            y.append(float(p[1]))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x, y = file_read('1in_linear.txt')\n",
    "\n",
    "print (\"col x - \" + str(x)) \n",
    "print (\"col y - \" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dlnet:\n",
    "    def _init(self, x, y):\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "        #holds input layer\n",
    "        self.X=x\n",
    "        \n",
    "        #holds desired output\n",
    "        self.Y=y\n",
    "        \n",
    "        #holds output of the network\n",
    "        #self.Y=np.zeros((1,self.Y.shape[1]))\n",
    "        \n",
    "        #programmable number of layers \n",
    "        self.L=2\n",
    "        \n",
    "        #define number of neurons ( position 1 - input, position 2 - number of neurons, \n",
    "        #Â position 3 = num of output layers)\n",
    "        self.dims = [2, 15, 1]\n",
    "        \n",
    "        #Dictionary that holds the weights and biases of each layer of the network \n",
    "        self.param = {}\n",
    "\n",
    "        #difference between expected vs actual\n",
    "        self.loss = []\n",
    "        \n",
    "        #learning rate \n",
    "        self.lr=1\n",
    "        \n",
    "        #number of training samples\n",
    "        self.sam = self.Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise variables of the network with random variables\n",
    "def nInit(self):\n",
    "    \n",
    "        #set random seed\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        # self.dims[1] = number of columns / number of hidden units of that layer \n",
    "        # self.dims[0] = number of features/rows of previous layer\n",
    "        self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0]) \n",
    "        \n",
    "        #Same number of rows as W1 but a single column\n",
    "        self.param['b1'] = np.zeros((self.dims[1], 1))        \n",
    "        \n",
    "        #self.dims[2] = rows = number of hidden units of that layer \n",
    "        #self.dims[0] = columns = number of rows of input \n",
    "        self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1]) \n",
    "        \n",
    "        #same number of rows as w2 but a single column\n",
    "        self.param['b2'] = np.zeros((self.dims[2], 1))                \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sigmoid function for activation \n",
    "def Sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relu function for activation \n",
    "def Relu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nloss(self,Yh):\n",
    "        loss = (1./self.sam) * (-np.dot(self.Y,np.log(Yh).T) - np.dot(1-self.Y, np.log(1-Yh).T))    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the forward pass step\n",
    "def forward(self):\n",
    "        \n",
    "        #first layer \n",
    "        \n",
    "        #multiply the weights of the first layer by the inouts and add the bias\n",
    "        Z1 = self.param['W1'].dot(self.X) + self.param['b1'] \n",
    "        \n",
    "        #apply activation function \n",
    "        A1 = Relu(Z1)\n",
    "        \n",
    "        #save variables\n",
    "        self.ch['Z1'],self.ch['A1']=Z1,A1\n",
    "        \n",
    "        \n",
    "        #second layer - take output from the last layer as input, repeat with different activation function\n",
    "        \n",
    "        Z2 = self.param['W2'].dot(A1) + self.param['b2']  \n",
    "        A2 = Sigmoid(Z2)\n",
    "        self.ch['Z2'],self.ch['A2']=Z2,A2\n",
    "        self.Yh=A2\n",
    "        loss=self.nloss(A2)\n",
    "        \n",
    "        print (self.Yh)\n",
    "        return self.Yh, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = dlnet._init(x,y,y)\n",
    "nn.lr=0.01\n",
    "nn.dims = [2, 15, 1]\n",
    "nn.gd(x, y, iter = 15000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
